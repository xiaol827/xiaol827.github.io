<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xiaolong Huang</title>

    <meta name="author" content="Xiaolong Huang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/rainbow-32.png">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Xiaolong Huang
                </p>
                <p>Hey, I am Xiaolong Huang (黄晓龙), an MSc student (Jan. 2025 - now) at MILA and Concordia University. Currently I am working on learned optimization (LO) under the supervision of Prof. <a href="https://eugenium.github.io">Eugene Belilovsky</a>.</p>
                <p>I am also deeply interested in exploring and interpreting visual and multimodal representations, especially in how to create faithful and robust representations and effectively leverage them for downstream learning. Always open to research discussions and collaborations, feel free to reach out!</p>
                <p style="text-align:center">
                  <a href="mailto:hirox827@gmail.com">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/XiaolongHuang_CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=9Yc727AAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp;/&nbsp;
<!--                   <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/xiaol827">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <img style="width:100%;max-width:100%" alt="profile photo" src="images/Ice_world.JPG" class="hoverZoomLink">
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/ELO.png" alt="microflakes" width="200" height="160"></td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="data/ELO.pdf"><papertitle>Towards Robust Unroll Generalization in Learned Optimizers</papertitle></a>
          <br>
          <strong>Xiaolong Huang</strong>, Benjamin Therien, Eugene Belilovsky
          <p></p>
          NeurIPS 2025 OPT Workshop
          <br>
          <a href="data/ELO.pdf">paper</a>
          <p></p>
        </td>
        </tr>

        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/Micro_Action_Recognition.png" alt="microflakes" width="200" height="160"></td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://dl.acm.org/doi/abs/10.1145/3606042.3616458"><papertitle>Advancing Micro-Action Recognition with Multi-Auxiliary Heads and Hybrid Loss Optimization</papertitle></a>
          <br>
          Qiankun Li, <strong>Xiaolong Huang</strong>, Huabao Chen, Feng He, Qiupu Chen, Zengfu Wang
          <p></p>
          <em>ACMMM</em>, 2024
          <br>
          <a href="https://dl.acm.org/doi/pdf/10.1145/3664647.3688975">paper</a>
          /
          <a href="https://github.com/qklee-lz/ACMMM2024-MAC">code</a>
          <p></p>
        </td>
        </tr>
            
        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/OLOR-Method.png" alt="microflakes" width="200" height="160"></td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="data/OLOR-AAAI.pdf"><papertitle>One Step Learning, One Step Review</papertitle></a>
          <br>
          <strong>Xiaolong Huang</strong>, Qiankun Li, Xueran Li, Xuesong Gao
          <p></p>
          <em>AAAI</em>, 2024
          <br>
          <a href="https://arxiv.org/abs/2401.10962">paper</a>
          /
          <a href="https://github.com/xiaol827/OLOR-AAAI-2024">code</a>
          <!-- /
          <a href="https://arxiv.org/abs/2401.10962">paper</a> -->
          <p></p>
        </td>
        </tr>
        
        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/TSN.png" alt="microflakes" width="200" height="160"></td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://dl.acm.org/doi/abs/10.1145/3606042.3616458"><papertitle>Mitigating Context Bias in Action Recognition via Skeleton-Dominated Two-Stream Network</papertitle></a>
          <br>
          Qiankun Li, <strong>Xiaolong Huang</strong>, Yuwen Luo, Xiaoyu Hu, Xinyu Sun, Zengfu Wang
          <p></p>
          <em>AMC-SME Workshop, ACMMM</em>, 2023 <font color="red"><strong>(Best Student Paper Award)</strong></font>
          <br>
          <a href="https://dl.acm.org/doi/pdf/10.1145/3606042.3616458">paper</a>
          <p></p>
        </td>
        </tr>
        
        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/MVM.png" alt="microflakes" width="200" height="160"></td>
<!--           <div class="one">
            <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/MVM.png" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images' width="160">
          </div>
          <script type="text/javascript">
            function zipnerf_start() {
              document.getElementById('zipnerf_image').style.opacity = "1";
            }

            function zipnerf_stop() {
              document.getElementById('zipnerf_image').style.opacity = "0";
            }
            zipnerf_stop()
          </script> -->
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://dl.acm.org/doi/pdf/10.1145/3581783.3612496"><papertitle>Data-Efficient Masked Video Modeling for Self-supervised Action Recognition</papertitle></a>
          <br>
          Qiankun Li, <strong>Xiaolong Huang</strong>, Zhifan Wan, Lanqing Hu, Shuzhe Wu, Jie Zhang, Shiguang Shan, Zengfu Wang
          <p></p>
          <em>ACMMM</em>, 2023
          <br>
          <a href="https://dl.acm.org/doi/pdf/10.1145/3581783.3612496">paper</a>
          <p></p>
        </td>
        </tr>

        <!-- <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/DT.png" alt="microflakes" width="200" height="160"></td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://ieeexplore.ieee.org/abstract/document/10361546"><papertitle>Embracing Large Natural Data: Enhancing Medical Image Analysis via Cross-domain Fine-tuning</papertitle></a>
          <br>
          Qiankun Li, <strong>Xiaolong Huang</strong>, Bo Fang, Huabao Chen, Siyuan Ding, Xu Liu
          <p></p>
          <em>JBHI</em>, 2023
          <br>
          <a href="https://dl.acm.org/doi/pdf/10.1145/3581783.3612496">webpage</a>
          <p></p>
        </td>
        </tr> -->

        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/SL_ILR.png" alt="microflakes" width="200" height="160"></td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="data/ILR_Workshop_ECCV_2nd_Place_Solution.pdf"><papertitle>2nd Place Solution to Google Universal Image Embedding</papertitle></a>
          <br>
          <strong>Xiaolong Huang</strong>, Qiankun Li
          <p></p>
          <em>ILR Workshop, ECCV</em>, 2022 <font color="red"><strong>(Oral)</strong></font>
          <br>
          <a href="https://arxiv.org/abs/2210.08735">paper</a>
          /
          <a href="https://github.com/xiaol827/ECCV2022-ILR-workshop">code</a>
          /
          <a href="https://www.kaggle.com/competitions/google-universal-image-embedding/leaderboard">Competition Link</a>
          <p></p>
        </td>
        </tr>
        
        
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:left;font-size:small;">
                  Folked from Jon Barron's <a href="https://github.com/jonbarron/jonbarron_website">website</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
